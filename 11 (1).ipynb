{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9a1b6-2297-4c63-8d45-cb0da5064656",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:1\n",
    "\n",
    "Ridge Regression, also known as Tikhonov regularization or L2 regularization, is a linear regression technique that aims to improve the stability and generalization performance of a model by adding a penalty term to the ordinary least squares (OLS) regression objective function. This penalty term is proportional to the squared magnitudes of the coefficients, which encourages the model to produce smaller coefficient values. The result is a trade-off between fitting the data and preventing large coefficients, which can help mitigate issues like multicollinearity and overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcd564-77d0-4bad-ab90-ec85884cce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:2\n",
    "The key assumptions of Ridge Regression are as follows:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be linear. Ridge Regression, like OLS regression, assumes that the true underlying relationship is linear.\n",
    "\n",
    "Independence of Errors: The errors (residuals) should be independent of each other. This assumption ensures that one error does not predict another.\n",
    "\n",
    "Homoscedasticity: The errors should have constant variance across all levels of the independent variables. Ridge Regression assumes that the variance of errors is the same regardless of the values of the predictors.\n",
    "\n",
    "No Perfect Multicollinearity: Ridge Regression, like OLS regression, assumes that there is no perfect multicollinearity among the predictor variables. However, Ridge Regression is less sensitive to multicollinearity compared to OLS regression due to the regularization effect that helps stabilize coefficient estimates.\n",
    "\n",
    "Normality of Errors: Ridge Regression, like OLS regression, assumes that the errors are normally distributed. While this assumption is important for statistical inference in OLS, Ridge Regression's primary goal is regularization and improved generalization, so the normality assumption might be less critical.\n",
    "\n",
    "Zero Mean of Residuals: The residuals should have a mean of zero, which ensures that the model is not systematically biased.\n",
    "\n",
    "It's important to note that Ridge Regression, due to its regularization nature, is somewhat robust to violations of the assumptions of normality and homoscedasticity. The regularization term can help mitigate the impact of deviations from these assumptions. Additionally, Ridge Regression is often used in scenarios where multicollinearity is a concern because it can stabilize coefficient estimates and reduce their sensitivity to changes in the data.\n",
    "\n",
    "While Ridge Regression can offer benefits in terms of improved model stability and generalization, it's essential to evaluate the assumptions and limitations in the context of your specific problem and dataset. Regularization techniques, including Ridge Regression, should be used thoughtfully and in conjunction with thorough exploratory data analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd59aba-7e5c-46ec-98c6-19a4ea989905",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:3\n",
    "Cross-Validation:\n",
    "Cross-validation is a widely used technique to assess how well a model generalizes to new, unseen data. The process involves splitting the dataset into multiple folds, training the Ridge Regression model on a subset of the data, and evaluating its performance on the remaining fold.\n",
    "\n",
    "Grid Search:\n",
    "Grid search involves specifying a range of λ values and evaluating the model's performance for each value within that range. \n",
    "\n",
    "Randomized Search:\n",
    "Similar to grid search, randomized search involves sampling λ values from a specified range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac5ba8-6de6-450f-a69d-88bbd7cee839",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:4\n",
    "\n",
    "Cross-Validation:\n",
    "Cross-validation is a widely used technique to assess how well a model generalizes to new, unseen data. The process involves splitting the dataset into multiple folds, training the Ridge Regression model on a subset of the data, and evaluating its performance on the remaining fold. This process is repeated multiple times, rotating which fold is used for testing. \n",
    "\n",
    "Grid Search:\n",
    "Grid search involves specifying a range of \n",
    "λ values and evaluating the model's performance for each value within that range. The \n",
    "λ value that leads to the best performance on the validation set is chosen. Grid search is often combined with cross-validation to provide a more robust estimate of performance.\n",
    "\n",
    "Randomized Search:\n",
    "Similar to grid search, randomized search involves sampling λ values from a specified range. However, instead of evaluating all values, only a random subset is evaluated. This approach can be computationally less intensive while still providing a reasonable selection of \n",
    "λ values to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec135740-5e4e-4e94-abc2-160f02caac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:5\n",
    "\n",
    "Shrinking Coefficients: Ridge Regression adds a penalty term to the objective function that encourages the model to have smaller coefficients. As λ increases, coefficients are pushed closer to zero. Features that contribute less to the model's fit will have smaller coefficients, but they are unlikely to be completely excluded.\n",
    "\n",
    "Relative Importance: Ridge Regression doesn't eliminate features entirely, but it assigns less importance to less relevant features. This can be useful when you want to retain some level of information from all features, even if they are not the most influential.\n",
    "\n",
    "Regularization Path: Ridge Regression often provides a \"regularization path\" that shows how coefficient values change as \n",
    "λ varies. By examining this path, you can identify features whose coefficients decrease significantly as λ increases. Features with small coefficients at high λ values may be candidates for removal.\n",
    "\n",
    "Trade-Off: Ridge Regression's strength lies in its ability to manage multicollinearity and reduce overfitting. It's particularly useful when you have a large number of correlated features, as it can help stabilize coefficient estimates. However, if your main goal is aggressive feature selection, Lasso Regression might be a more suitable choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad8f14-850f-43d4-bb10-b88f0da75483",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:6\n",
    "\n",
    "Here's how Ridge Regression performs in the presence of multicollinearity:\n",
    "\n",
    "Stabilization of Coefficients: In the presence of multicollinearity, OLS regression coefficient estimates can be highly sensitive to small changes in the data. Ridge Regression reduces the magnitude of coefficients by adding a penalty term proportional to the sum of squared coefficients. This regularization effect helps stabilize the coefficient estimates, making them less sensitive to multicollinearity.\n",
    "\n",
    "Bias-Variance Trade-Off: Multicollinearity can increase the variance of coefficient estimates, leading to overfitting. Ridge Regression's penalty term reduces the magnitude of the coefficients, which can help prevent overfitting by trading off some bias for reduced variance.\n",
    "\n",
    "Shrinking Highly Correlated Coefficients: When predictors are highly correlated, OLS regression might assign large coefficients to one of the correlated predictors while assigning small coefficients to others. Ridge Regression tends to shrink the coefficients of correlated predictors towards each other. This is beneficial when you want the model to acknowledge the shared predictive power of correlated features.\n",
    "\n",
    "Multicollinearity Management: Ridge Regression doesn't eliminate features entirely due to the squared L2 penalty term. However, it reduces their influence, which can be advantageous when retaining all features is necessary, such as in situations where each feature has domain relevance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
